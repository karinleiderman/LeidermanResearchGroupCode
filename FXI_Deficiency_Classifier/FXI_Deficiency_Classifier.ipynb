{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e700b58",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83f26d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, auc\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt  # Import matplotlib\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import ast\n",
    "from scipy import stats\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler, Normalizer, RobustScaler# Import StandardScaler\n",
    "pd.options.display.max_rows = 3500  # Set to the desired number of rows\n",
    "\n",
    "#if running on google colab, uncomment the next two lines\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive') # Mount Google Drive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce69144",
   "metadata": {},
   "source": [
    "## Paths to Data Files\n",
    "- Data files should be in the same folder as the python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5212be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_man ='DAPs_Data.xlsx'\n",
    "file_path_gidleynoflat = '250203SER_Indeterminants_Gidley_noflatliners_17samps.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4035fa26",
   "metadata": {},
   "source": [
    "## Function Definitions\n",
    "Defines functions to:\n",
    "- Find the optimal threshold \n",
    "- Evaluate the Logisitic Regression Model given coeffiecients and intercept\n",
    "- Compute Confusion Matrix Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fb2c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the optimal threshold that maximizes Youden's J statistic.\n",
    "def optimal_threshold_youden(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    Finds the optimal threshold that maximizes Youden's J statistic.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): True binary labels.\n",
    "        y_scores (array-like): Predicted probabilities or scores.\n",
    "\n",
    "    Returns:\n",
    "        float: The optimal threshold.\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    j_scores = tpr - fpr\n",
    "    optimal_idx = np.argmax(j_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    return optimal_threshold\n",
    "\n",
    "#Use found coeffiecients and intercept in logisitic regression model. \n",
    "def logistic_regression(coefficients, intercept, df):\n",
    "    \"\"\"\n",
    "    Logistic regression prediction for the given data pulled from a DataFrame\n",
    "    \n",
    "    Parameters: \n",
    "    coefficients (list): Coefficients for each feature (one value per feature).\n",
    "    intercept (float): Intercept of the logistic regression model.\n",
    "    df (pandas.DataFrame): DataFrame containing the input data for which predictions are to be made\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: Predicted probabilities for each observation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the DataFrame is a numpy array of feature values\n",
    "    X = df.to_numpy() #Converts DataFrame to a 2D numpy array\n",
    "    \n",
    "    # Calculate the linear combination of coefficients and features: X * coefficients + intercept\n",
    "    linear_combination = np.dot(X,np.array(coefficients)) + intercept\n",
    "    \n",
    "    # Apply the logistic (sigmoid) function\n",
    "    predictions = 1/(1+np.exp(-linear_combination))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "#Compute confusion matrix and related metrics\n",
    "def compute_confusion_metrics(y_true, y_scores,threshold):\n",
    "    \"\"\"\n",
    "    Compute confusion matrix (TN, FP, FN, TP) and related metrics (FNR, TPR, TNR, FPR) at a given \n",
    "    classification threshold.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (array-like): Ground-truth binary labels (0 or 1).\n",
    "    y_scores (array-like): Predicted probabilities or decision scores.\n",
    "    threshold (float):Decision threshold for converting scores to binary predictions.\n",
    "\n",
    "    Returns:\n",
    "    tuple: (tn, fp, fn, tp, fnr, tpr, tnr, fpr)\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction = (y_scores >= threshold).astype(int)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, prediction).ravel()\n",
    "        \n",
    "    fnr = fn/(tp+fn) #false negative rate - proportion of 1's classifed as 0, sensitivity = 1 - fnr \n",
    "    tpr = tp/(tp+fn) #true positive rate - proportion of 1's classifed as 1, sensitivity\n",
    "    \n",
    "    tnr = tn/(tn+fp) #true negative rate - proportion of 0's classified as 0, specificity = tnr\n",
    "    fpr = fp/(tn+fp) #false negative rate - proportion of 0's classified as 1, specificity = 1- tnr\n",
    "    \n",
    "    \n",
    "    return tn, fp, fn, tp, fnr, tpr, tnr, fpr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0015fa68",
   "metadata": {},
   "source": [
    "## Load in Datasets and Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the data with column titles\n",
    "# Replace 'your_file.xlsx' with the actual path to your Excel file\n",
    "data_man = pd.read_excel(file_path_man, header=0)\n",
    "data_gidleynoflat = pd.read_excel(file_path_gidleynoflat, header=0)  \n",
    "\n",
    "##################################################\n",
    "# 2. Separate features and target variable\n",
    "X_man = data_man.drop(['Phenotype', 'Patient Number'], axis=1) # Drop both phenotype and Patient Number columns\n",
    "y_man = data_man['Phenotype']\n",
    "id_man = data_man['Patient Number']\n",
    "\n",
    "#*****************\n",
    "X_gidleynoflat = data_gidleynoflat.drop(['Phenotype', 'Patient Number'], axis=1) # Drop both phenotype and Patient Number columns\n",
    "y_gidleynoflat = data_gidleynoflat['Phenotype']\n",
    "\n",
    "#*********************\n",
    "# 3. Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_man)\n",
    "#uncomment to store the scaling\n",
    "# scaling_params = pd.DataFrame({\n",
    "#     'feature_index': X_man.columns, \n",
    "#     'data_min': scaler.data_min_,\n",
    "#     'data_max': scaler.data_max_,\n",
    "#     'data_range': scaler.data_range_,\n",
    "#     'scale': scaler.scale_,\n",
    "#     'min_offset': scaler.min_\n",
    "# })\n",
    "# scaling_params.to_csv('FXI_minmax_scaling_parameters.csv', index=False)\n",
    "\n",
    "#***************** \n",
    "X_man_scaled = scaler.transform(X_man) # Fit and transform on the entire feature set for the manchester data\n",
    "X_gidleynoflat_scaled = scaler.transform(X_gidleynoflat) # Fit and transform on the entire feature set\n",
    "\n",
    "# 4. Convert into dataframes\n",
    "X_man_scaled = pd.DataFrame(X_man_scaled, columns=X_man.columns, index=X_man.index) # Convert X_scaled back to DataFrame\n",
    "X_gidleynoflat_scaled = pd.DataFrame(X_gidleynoflat_scaled, columns=X_gidleynoflat.columns, index=X_gidleynoflat.index) # Convert X_scaled back to DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a25e219",
   "metadata": {},
   "source": [
    "## User defines inputs\n",
    "User defines:\n",
    "- Fixed features (features that should be included in every model constructed)\n",
    "- Output file name (default is test.csv if none is given)\n",
    "- Number of features to be used in model\n",
    "## Find Model for each combination and analyze\n",
    "Finds the best logistic regression model for each combination using 5-fold cross-validation.\n",
    "For each model, \n",
    "- Compute 95% Confidence Interval and mean AUROC using bootstrapping\n",
    "- Compute the optimal threshold used for classification by maximizing the Youden-J statisitc\n",
    "- Find the Model Area Under Receiver Operating Characteristic Curve (AUROC)\n",
    "- Compute the confusion matrix \n",
    "    - TP (true positive): Bleeder is classified as Bleeder\n",
    "    - FN (false negative): Bleeder is classified as Non-Bleeder\n",
    "    - FP (false positive): Non-Bleeder is classified as Bleeder\n",
    "    - TN (true negative): Non-Bleeder is classified as Non-Bleeder\n",
    "- Compute performance rates:\n",
    "    - TPR (true positive rate): Proportion of Bleeders correctly identified → TP / (TP + FN)\n",
    "    - FNR (false negative rate): Proportion of Bleeders missed (classified as Non-Bleeders) → FN / (TP + FN)\n",
    "    - FPR (false positive rate): Proportion of Non-Bleeders incorrectly identified as Bleeders → FP / (TN + FP)\n",
    "    - TNR (true negative rate): Proportion of Non-Bleeders correctly identified → TN / (TN + FP)\n",
    "\n",
    "### Outputs:\n",
    "- Excel File containing results \n",
    "- Displays results for the top 5 models (sorted by mean AUROC)\n",
    "- Plot of the Receiver Operating Character (ROC) curves for the top 5 models (sorted by AUROC) for:\n",
    "    - Manchester Data\n",
    "    - Indeterminant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6f99a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "#################### Section for User Inputs ######################\n",
    "###################################################################\n",
    "\n",
    "# 1. Get number of fixed features from user input\n",
    "fixed_features = []\n",
    "num_fixed_features = int(input(\"Enter the number of fixed features (0, 1 or 2): \"))\n",
    "\n",
    "# 2. Get output filename from user (will add .csv later if not added here) \n",
    "output_file_name = input(\"Enter the name for the output csv file (e.g., my_results.csv): \") \n",
    "\n",
    "#If output_file_name is left empty, test.csv will be used as the output filename\n",
    "if not output_file_name:\n",
    "    output_file_name = 'test'\n",
    "    print('\\tNo output file name given. Default filename = test.csv will be used instead.')\n",
    "\n",
    "#Add .csv extension to output_file_name if it's not already present\n",
    "if not output_file_name.lower().endswith((\".csv\")):\n",
    "    output_file_name += \".csv\"\n",
    "\n",
    "# 3. Generate feature combinations to use in models \n",
    "if num_fixed_features == 0: #if there are no fixed features\n",
    "    protein_combinations = []\n",
    "    num_additional_features = int(input(\"Enter the number of features to combine: \")) #get num_additonal_features\n",
    "    for combination in combinations(X_man.columns, num_additional_features):\n",
    "        protein_combinations.append(list(combination)) #generate combinations of num_additional_features features \n",
    "else: #if user wants fixed features\n",
    "    for i in range(num_fixed_features):\n",
    "        feature_name = input(f\"Enter the name of fixed feature {i+1}: \")\n",
    "        fixed_features.append(feature_name)\n",
    "\n",
    "    # Get number of additional features for combinations\n",
    "    num_additional_features = int(input(\"Enter the number of additional features to combine: \")) \n",
    "\n",
    "    # Generate combinations with fixed features\n",
    "    protein_combinations = []\n",
    "    remaining_features = [col for col in X_man.columns if col not in fixed_features]\n",
    "    for combination in combinations(remaining_features, num_additional_features):\n",
    "        protein_combinations.append(fixed_features + list(combination))\n",
    "        \n",
    "##########################################################################\n",
    "###################### End of User Defined Inputs ########################\n",
    "##########################################################################\n",
    "\n",
    "##########################################################################\n",
    "########    Perform logistic regression with cross-validation,    ########\n",
    "######## get confidence intervals, and calculate AUCs & threshold ########\n",
    "##########################################################################\n",
    "results = []\n",
    "\n",
    "for combination in protein_combinations:\n",
    "     #select data columns for chosen features\n",
    "    X_man_subset = X_man_scaled[combination] \n",
    "    X_gidleynoflat_subset = X_gidleynoflat_scaled[combination]\n",
    "\n",
    "    # 1. 5-fold Cross-Validation for Model Selection\n",
    "    cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=0)\n",
    "    \n",
    "    best_model_cv = None  # To store the best model from CV\n",
    "    best_auc_cv = 0  # To store the best AUC from CV\n",
    "\n",
    "    for train_index, test_index in cv.split(X_man_subset, y_man):\n",
    "        X_man_train, X_man_test = X_man_subset.iloc[train_index], X_man_subset.iloc[test_index]\n",
    "        y_man_train, y_man_test = y_man.iloc[train_index], y_man.iloc[test_index]\n",
    "\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_man_train, y_man_train)\n",
    "        y_man_pred_proba = model.predict_proba(X_man_test)[:, 1]\n",
    "        auc_score_man = roc_auc_score(y_man_test, y_man_pred_proba)\n",
    "\n",
    "        if auc_score_man > best_auc_cv:\n",
    "            best_auc_cv = auc_score_man\n",
    "            best_model_cv = model  # Store the best model from this fold\n",
    "            \n",
    "    # 2. Bootstrapping for the Best Model from Cross-Validation\n",
    "    n_bootstraps = 1000\n",
    "    bootstrap_auc_scores = []\n",
    "    \n",
    "    if best_model_cv is not None:  # Check if a best model was found in CV\n",
    "        for _ in range(n_bootstraps):\n",
    "            X_boot, y_boot = resample(X_man_subset, y_man, stratify=y_man, replace=True)\n",
    "            # Use the best model directly (no retraining or subsetting)\n",
    "            y_pred_proba_boot = best_model_cv.predict_proba(X_boot)[:, 1]\n",
    "            bootstrap_auc_scores.append(roc_auc_score(y_boot, y_pred_proba_boot))\n",
    "            \n",
    "        mean_auc = np.mean(bootstrap_auc_scores)\n",
    "        confidence_interval = np.percentile(bootstrap_auc_scores, [2.5, 97.5]) #95% CI\n",
    "        saving_CI = f\"[{round(confidence_interval[0],3)} - {round(confidence_interval[1],3)}]\" #write it to be stored in results\n",
    "\n",
    "        # 3. Apply Best Model to the Datasets and Calculate Results\n",
    "        y_man_pred_proba = best_model_cv.predict_proba(X_man_scaled[combination])[:, 1]\n",
    "        y_gidleynoflat_pred_proba = best_model_cv.predict_proba(X_gidleynoflat_scaled[combination])[:, 1]        \n",
    "        \n",
    "        #Calculate AUC for each\n",
    "        auc_man = roc_auc_score(y_man, y_man_pred_proba)\n",
    "        auc_gidleynoflat = roc_auc_score(y_gidleynoflat, y_gidleynoflat_pred_proba)\n",
    "        \n",
    "        #Find optimal threshold by optimizing youden-j statistic\n",
    "        youden_threshold = optimal_threshold_youden(y_man, y_man_pred_proba)\n",
    "        \n",
    "        #Calculate confusion matrix (TP,TN, FP, FN), FNR, TPR, TNR, FPR\n",
    "        tn_man, fp_man, fn_man, tp_man, fnr_man, tpr_man, tnr_man, fpr_man = compute_confusion_metrics(y_man, y_man_pred_proba,youden_threshold)\n",
    "        tn_gidleynoflat, fp_gidleynoflat, fn_gidleynoflat, tp_gidleynoflat, fnr_gidleynoflat, tpr_gidleynoflat, tnr_gidleynoflat, fpr_gidleynoflat = compute_confusion_metrics(y_gidleynoflat, y_gidleynoflat_pred_proba,youden_threshold)\n",
    "\n",
    "        # 4. Store the Results\n",
    "        results.append({\n",
    "            'combination': combination,\n",
    "            #Results from Manchester Dataset\n",
    "            'AUROC_manchester': round(auc_man,3), #Auc for full Manchester dataset\n",
    "            'mean_AUROC': round(mean_auc,3), #Mean auc for Manchester dataset calcualted during bootstrapping\n",
    "            'confidence_interval': saving_CI, #95% confidence_interval,\n",
    "            'TrueNeg_manchester': tn_man,\n",
    "            'FalsePos_manchester': fp_man,\n",
    "            'FalseNeg_manchester': fn_man,\n",
    "            'TruePos_manchester': tp_man,\n",
    "            'FNR_manchester': fnr_man,\n",
    "            'TPR_manchester': tpr_man,\n",
    "            'TNR_manchester': tnr_man,\n",
    "            'FPR_manchester': fpr_man,\n",
    "            #Results from Indeterminant (Gidley_noflat) dataset\n",
    "            'AUROC_gidleynoflat': round(auc_gidleynoflat,3), \n",
    "            'TrueNeg_gidleynoflat': tn_gidleynoflat,\n",
    "            'FalsePos_gidleynoflat': fp_gidleynoflat,\n",
    "            'FalseNeg_gidleynoflat': fn_gidleynoflat,\n",
    "            'TruePos_gidleynoflat': tp_gidleynoflat,\n",
    "            'FNR_gidleynoflat': fnr_gidleynoflat,\n",
    "            'TPR_gidleynoflat': tpr_gidleynoflat,\n",
    "            'TNR_gidleynoflat': tnr_gidleynoflat,\n",
    "            'FPR_gidleynoflat': fpr_gidleynoflat,\n",
    "            #Model Information\n",
    "            'best_model_cv': best_model_cv,  #Store the best model from CV\n",
    "            'model_intercept': best_model_cv.intercept_[0],  #Intercept from best CV model\n",
    "            'model_coefficients': best_model_cv.coef_.tolist(),  #Coefficients from best CV model\n",
    "            'optimal_thresh': youden_threshold #Optimal threshold\n",
    "        })\n",
    "\n",
    "# 5. Reformat and Save Results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by=['mean_AUROC'], ascending=False) #sort by the mean_auc\n",
    "results_df.head(10)\n",
    "\n",
    "save_folder = 'Results'\n",
    "\n",
    "results_df.to_csv(os.path.join(save_folder, output_file_name), index=False)\n",
    "\n",
    "print(' ')\n",
    "print(f\"Results saved to: {save_folder}\")\n",
    "print(' ')\n",
    "\n",
    "# 6. Plot ROC curves for top 10\n",
    "top_5_results = results_df.head(5) \n",
    "print(top_5_results)\n",
    "for index, row in top_5_results.iterrows():\n",
    "    combination = row['combination']\n",
    "    X_man_subset = X_man_scaled[combination]\n",
    "\n",
    "    # Create LogisticRegression model with default parameters\n",
    "    best_model = row['best_model_cv']  # Get the best model from CV\n",
    "    roc_auc  = row['AUROC_manchester']\n",
    "    y_man_pred_proba = best_model.predict_proba(X_man_scaled[combination])[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_man, y_man_pred_proba)  # Calculate ROC curve\n",
    "    plt.plot(fpr, tpr,lw=3,label=f'{combination} (AUROC = {roc_auc:.3f})')  # Plot ROC curve\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1 - Specificity', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Sensitivity', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=12)\n",
    "plt.title(\"ROC Curve - Manchester Dataset\", fontsize=18, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for index, row in top_5_results.iterrows():\n",
    "    combination = row['combination']\n",
    "    X_gidleynoflat_subset = X_gidleynoflat_scaled[combination]\n",
    "\n",
    "    # Create LogisticRegression model with default parameters\n",
    "    best_model = row['best_model_cv']  # Get the best model from CV\n",
    "    roc_auc  = row['AUROC_gidleynoflat']\n",
    "    y_gidleynoflat_pred_proba = best_model.predict_proba(X_gidleynoflat_scaled[combination])[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_gidleynoflat, y_gidleynoflat_pred_proba)  # Calculate ROC curve\n",
    "    plt.plot(fpr, tpr,lw=3,label=f'{combination} (AUROC = {roc_auc:.3f})')  # Plot ROC curve\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1 - Specificity', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Sensitivity', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=12)\n",
    "plt.title(\"ROC Curve - Indeterminant Dataset\", fontsize=18, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baed78b",
   "metadata": {},
   "source": [
    "# Optinial Code to generate Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d064cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probabilties(X_data_scaled, y_true, model_df):\n",
    "     \"\"\"\n",
    "    Compute predicted probabilities for multiple logistic regression models.\n",
    "\n",
    "    For each model in `model_df`, selects the feature subset, applies the\n",
    "    model coefficients and intercept, and calculates probabilities for all\n",
    "    samples. Returns a DataFrame with true labels and predicted probabilities\n",
    "    for each feature combination.\n",
    "    \n",
    "    Parameters:\n",
    "    X_data_scaled (pandas.DataFrame): Scaled feature data for all samples.\n",
    "    y_true (array-like): True labels for the samples (used as reference in the results DataFrame).\n",
    "    model_df (pandas.DataFrame): DataFrame containing model specifications with columns:\n",
    "        - 'combination': list of feature names used in the model\n",
    "        - 'model_intercept': intercept term\n",
    "        - 'model_coefficients': coefficients for each feature\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing:\n",
    "        - 'Phenotype' column with true labels\n",
    "        - One column per feature combination with predicted probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    results_prob = pd.DataFrame({'Phenotype': y_true})\n",
    "    for index,row in model_df.iterrows():\n",
    "        comb = ast.literal_eval(row['combination'])\n",
    "        X_data_subset = X_data_scaled[comb]\n",
    "#         row_index = results_df.index[results_df['combination'] == str(comb)].tolist()\n",
    "#         row_index = row_index[0]\n",
    "        intercept = model_df['model_intercept'][index]\n",
    "        coeff = model_df['model_coefficients'][index]\n",
    "        coeff = ast.literal_eval(coeff)[0]\n",
    "        y_proba = logistic_regression(coeff, intercept, X_data_subset)\n",
    "        results_prob[str(comb)] = y_proba\n",
    "    \n",
    "    return results_prob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
